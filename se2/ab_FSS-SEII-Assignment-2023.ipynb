{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcb2a67b",
   "metadata": {},
   "source": [
    "# Fundamentals of Software Systems (FSS)\n",
    "**Software Evolution â€“ Part 02 Assignment**\n",
    "\n",
    "## Submission Guidelines\n",
    "\n",
    "To correctly complete this assignment you must:\n",
    "\n",
    "* Carry out the assignment in a team of 2 to 4 students.\n",
    "* Carry out the assignment with your team only. You are allowed to discuss solutions with other teams, but each team should come up its own personal solution. A strict plagiarism policy is going to be applied to all the artifacts submitted for evaluation.\n",
    "* As your submission, upload the filled Jupyter Notebook (including outputs) together with the d3 visualization web pages (i.e. upload everything you downloaded including the filled Jupyter Notebook plus your `output.json`)\n",
    "* The files must be uploaded to OLAT as a single ZIP (`.zip`) file by 2023-12-04 18:00.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a552ae3",
   "metadata": {},
   "source": [
    "## Group Members\n",
    "* Adam Bauer, 20-744-964\n",
    "* Alessio Brazeol TODO\n",
    "* Paul TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46791aa",
   "metadata": {},
   "source": [
    "## Task Context\n",
    "\n",
    "In this assigment we will be analyzing the _elasticsearch_ project. All following tasks should be done with the subset of commits from tag `v1.6.0` to tag `v2.0.0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install plotly\n",
    "%pip install tdqm\n",
    "%pip install nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydriller import Repository, Git\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import platform\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clone the project -> easier setup (will not clone if folder already exists or is not empty)\n",
    "url = \"https://github.com/elastic/elasticsearch\"\n",
    "repo_path = os.path.join(os.getcwd(), 'elasticsearch')\n",
    "clone = f\"git clone {url} {repo_path}\" \n",
    "\n",
    "os.system(clone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing repo from v1.6.0 to v2.0.0, which are commits from 2015-06-09 13:35:08+00:00 to 2015-10-21 23:01:03+02:00\n"
     ]
    }
   ],
   "source": [
    "# define Repository\n",
    "gr = Git(repo_path)\n",
    "from_tag = \"v1.6.0\"\n",
    "to_tag = \"v2.0.0\"\n",
    "since_time = gr.get_commit_from_tag(from_tag).committer_date\n",
    "to_time = gr.get_commit_from_tag(to_tag).committer_date\n",
    "\n",
    "repo = Repository(repo_path, since=since_time, to=to_time)\n",
    "print(f\"Analyzing repo from {from_tag} to {to_tag}, which are commits from {since_time} to {to_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002e25f-8759-4897-9bc9-d26f1880b4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum \n",
    "\n",
    "class Modification(Enum):\n",
    "    ADDED = \"Lines added\"\n",
    "    REMOVED = \"Lines removed\"\n",
    "    TOTAL = \"Lines added + lines removed\"\n",
    "    DIFF = \"Lines added - lines removed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a29d1b",
   "metadata": {},
   "source": [
    "## Task 1: Author analysis\n",
    "\n",
    "In the following, please consider only `java` files.\n",
    "\n",
    "The first task is to get an overview of the author ownership of the _elasticsearch_ project. In particular, we want to understand who are the main authors in the system between the two considered tags, the authors distribution among files and the files distribution among authors. To this aim, perform the following:\n",
    "\n",
    "* create a dictionary (or a list of tuples) with the pairs author => number of modified files\n",
    "* create a dictionary (or a list of tuples) with the pairs file => number of authors who modified the file\n",
    "* visualize the distribution of authors among files: the visualization should have on the x axis the number of authors per file (from 1 to max), and on the y axis the number of files with the given number of authors (so for example the first bar represent the number of files with single author)\n",
    "* visualize the distribution of files among authors: the visualization should have on the x axis the number of files per author (from 1 to max), and on the y axis the number of authors that own the given number of files (so for example the first bar represent the minor contributors, i.e., the number of authors who own 1 file)\n",
    "\n",
    "Comment the two distribution visualizations.\n",
    "\n",
    "\n",
    "\n",
    "Now, let's look at the following 3 packages in more details:\n",
    "\n",
    "1. `src/main/java/org/elasticsearch/common`\n",
    "2. `src/main/java/org/elasticsearch/rest`\n",
    "3. `src/main/java/org/elasticsearch/cluster`\n",
    "\n",
    "Create a function that, given the path of a package and a modification type (see class Modification below), returns a dictionary of authors => number, where the number counts the total lines added or removed or added+removed or added-removed (depending on the given Modification parameter), for the given package. To compute the value at the package level, you should aggregate the data per file.\n",
    "\n",
    "Using the function defined above, visualize the author contributions (lines added + lines removed). The visualization should have the author on the x axis, and the total lines on the y axis. Sort the visualization in decreasing amount of contributions, i.e., the main author should be the first.\n",
    "\n",
    "Compare the visualization for the 3 packages and comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9aafca-a8d4-4f16-8b22-df3950dc9f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(repo):\n",
    "    authors_dict = defaultdict(set)\n",
    "    files_dict = defaultdict(set)\n",
    "\n",
    "    commits = [commit for commit in repo.traverse_commits()]\n",
    "\n",
    "    for commit in tqdm(commits, desc=\"Analyzing commits\", unit=\"commit\"):\n",
    "        for file in commit.modified_files:\n",
    "           filename = file.new_path\n",
    "           \n",
    "           authors_dict[commit.author.name].add(filename) #since dict is a set, we don't have to care about duplicities\n",
    "           files_dict[filename].add(commit.author.name) #since dict is a set, we don't have to care about duplicities\n",
    "           \n",
    "    # perform counting on both dictionaries to get the numbers\n",
    "    authors_dict = {author: len(files) for author, files in authors_dict.items()}\n",
    "    files_dict = {file: len(authors) for file, authors in files_dict.items()}\n",
    "    return authors_dict, files_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualuse them\n",
    "authors_dict, files_dict = create_dictionary(repo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_chart(dictionary, title, ylabel, xlabel, top_n=10):\n",
    "    # Counting the occurrences in the dictionary\n",
    "    counts = Counter(dictionary.values())\n",
    "\n",
    "    # Getting the top n most common elements\n",
    "    top_counts = dict(counts.most_common())\n",
    "\n",
    "    # Preparing data for plotting\n",
    "    values = list(top_counts.keys())\n",
    "    frequencies = list(top_counts.values())\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(50, 5))\n",
    "    plt.bar( frequencies, values)\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel(xlabel)\n",
    "\n",
    "    # Adjusting x-ticks to show only labels for used values\n",
    "    plt.xticks([val for val in values if val in dictionary.values()])\n",
    "\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_interactive_bar_chart(dictionary, title, ylabel, xlabel, top_n=10):\n",
    "    # Counting the occurrences in the dictionary\n",
    "    counts = Counter(dictionary.values())\n",
    "\n",
    "    # Sorting the counts dictionary by values in descending order\n",
    "    sorted_counts = dict(sorted(counts.items(), key=lambda item: item[0]))\n",
    "\n",
    "    # Getting the top n most common elements after sorting\n",
    "\n",
    "    # Preparing data for plotting\n",
    "    values = list(sorted_counts.keys())\n",
    "    frequencies = list(sorted_counts.values())\n",
    "\n",
    "    # Creating an interactive bar chart\n",
    "    fig = px.bar(x=values, y=frequencies, labels={'x': xlabel, 'y': ylabel}, title=title)\n",
    "    fig.update_layout(xaxis={'type':'category'})\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "\n",
    "plot_interactive_bar_chart(files_dict, \"How many files are used by how many authors (so for example the first bar represent the number of files with single author)\", \"Number of authors\", \"Number of files\")\n",
    "plot_interactive_bar_chart(authors_dict, \"How many authors have interacted with how many files (so for example the first bar represent the minor contributors, i.e., the number of authors who own 1 file)\", \"Number of authors\", \"Number of files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO add comment for the visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modified_value(file, modification_type):\n",
    "    if(modification_type not in Modification):\n",
    "        raise ValueError(\"Invalid modification type\")\n",
    "    \n",
    "    if(modification_type == Modification.ADDED):\n",
    "        return file.added_lines\n",
    "    elif(modification_type == Modification.REMOVED):\n",
    "        return file.deleted_lines\n",
    "    elif(modification_type == Modification.TOTAL):\n",
    "        return file.added_lines + file.deleted_lines\n",
    "    elif(modification_type == Modification.DIFF):\n",
    "        return file.added_lines - file.deleted_lines\n",
    "\n",
    "def get_authors_numbers(package_paths, modification_types):\n",
    "    if(not isinstance(package_paths, list)):\n",
    "        raise ValueError(\"Package paths must be a list\")\n",
    "    if(not isinstance(modification_types, list)):\n",
    "        raise ValueError(\"Modification types must be a list\")\n",
    "    \n",
    "    # restrict repo to the previous defined tags\n",
    "    repo = Repository(repo_path, since=since_time, to=to_time)\n",
    "    commits = [commit for commit in repo.traverse_commits()]\n",
    "\n",
    "    authors_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0)))\n",
    "    for commit in tqdm(commits, desc=\"Analyzing commits\", unit=\"commit\"):\n",
    "        for file in commit.modified_files:\n",
    "            filename = file.new_path if file.new_path else file.old_path\n",
    "\n",
    "            for package_path in package_paths:\n",
    "                if filename.startswith(package_path):\n",
    "                    for modification_type in modification_types:\n",
    "                        authors_dict[package_path][modification_type][commit.author.name] += get_modified_value(file, modification_type)\n",
    "\n",
    "    return authors_dict   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be aware, they have moved from core into server folder in 2018\n",
    "packages = [\"core\\\\src\\\\main\\\\java\\\\org\\\\elasticsearch\\\\common\", \"core\\\\src\\\\main\\\\java\\\\org\\\\elasticsearch\\\\rest\", \"core\\\\src\\\\main\\\\java\\\\org\\\\elasticsearch\\\\cluster\"]\n",
    "modifications = [Modification.ADDED, Modification.REMOVED, Modification.TOTAL, Modification.DIFF]\n",
    "analysis_packages = get_authors_numbers(packages, modifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def graph(data):\n",
    "#     # sort array Author: number of lines\n",
    "#     data = dict(sorted(data.items(), key=lambda item: item[1]))\n",
    "#     # split into 2 arrays\n",
    "#     authors = data.keys()\n",
    "#     lines = data.values()\n",
    "#     # create bar chart\n",
    "#     plt.figure(figsize=(150, 5))\n",
    "#     plt.bar(authors, lines)\n",
    "#     plt.xticks(rotation=90)\n",
    "#     plt.show()\n",
    "\n",
    "import plotly.express as px\n",
    "def graph_interactive(data, title):\n",
    "    # Sort the dictionary by values (number of lines)\n",
    "    sorted_data = dict(sorted(data.items(), key=lambda item: item[1]))\n",
    "\n",
    "    # Splitting the sorted dictionary into authors and lines\n",
    "    authors = list(sorted_data.keys())\n",
    "    lines = list(sorted_data.values())\n",
    "\n",
    "    # Creating an interactive bar chart using Plotly\n",
    "    fig = px.bar(x=authors, y=lines)\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"Authors\",\n",
    "        yaxis_title=\"Number of Lines\",\n",
    "        xaxis={'categoryorder':'total descending'}\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "for pack in packages:\n",
    "    graph_interactive(analysis_packages[pack][Modification.TOTAL], \"Package: \"+pack)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo comment on the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9eee63",
   "metadata": {},
   "source": [
    "## Task 2: Knowledge loss\n",
    "\n",
    "We now want to analyze the knowledge loss when the main contributor of the analyzed project would leave. For this we will use the circle packaging layout introduced in the \"Code as a Crime Scene\" book. This assignment includes the necessary `knowledge_loss.html` file as well as the `d3` folder for all dependencies. Your task is to create the `output.json` file according to the specification below. This file can then be visualized with the files provided.\n",
    "\n",
    "For showing the visualization, once you have the output as `output.json` you should\n",
    "\n",
    "* make sure to have the `knowledge_loss.html` file in the same folder\n",
    "* start a local HTTP server in the same folder (e.g. with python `python3 -m http.server`) to serve the html file (necessary for d3 to work)\n",
    "* open the served `knowledge_loss.html` and look at the visualization\n",
    "\n",
    "Based on the visualization, comment on how is the project in terms of project loss and what could happen if the main contributor would leave.\n",
    "\n",
    "\n",
    "### Output Format for Visualization\n",
    "\n",
    "* `root` is always the root of the tree\n",
    "* `size` should be the total number of lines of contribution\n",
    "* `weight` can be set to the same as `size`\n",
    "* `ownership` should be set to the percentage of contributions from the main author (e.g. 0.98 for 98% if contributions coming from the main author)\n",
    "\n",
    "```\n",
    "{\n",
    "  \"name\": \"root\",\n",
    "  \"children\": [\n",
    "    {\n",
    "      \"name\": \"test\",\n",
    "      \"children\": [\n",
    "        {\n",
    "          \"name\": \"benchmarking\",\n",
    "          \"children\": [\n",
    "            {\n",
    "              \"author_color\": \"red\",\n",
    "              \"size\": \"4005\",\n",
    "              \"name\": \"t6726-patmat-analysis.scala\",\n",
    "              \"weight\": 1.0,\n",
    "              \"ownership\": 0.9,\n",
    "              \"children\": []\n",
    "            },\n",
    "            {\n",
    "              \"author_color\": \"red\",\n",
    "              \"size\": \"55\",\n",
    "              \"name\": \"TreeSetIterator.scala\",\n",
    "              \"weight\": 0.88,\n",
    "              \"ownership\": 0.9,\n",
    "              \"children\": []\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "{\n",
    "  \"name\": \"root\",\n",
    "  \"children\": [\n",
    "    {\n",
    "      \"name\": \".settings\",\n",
    "      \"children\": [\n",
    "        {\n",
    "              \"name\": \"org.eclipse.core.resources.prefs\",\n",
    "              \"val1\": 0,\n",
    "              \"val2\": 6,\n",
    "              \"children\": []\n",
    "        },\n",
    "        {\n",
    "              \"name\": \"org.eclipse.jdt.core.prefs\",\n",
    "              \"val1\": 0,\n",
    "              \"val2\": 18,\n",
    "              \"children\": []\n",
    "        },\n",
    "      \n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "      \"name\": \"core\",\n",
    "      \"children\": [\n",
    "          {\n",
    "                \"name\": \"license.txt\",\n",
    "                \"val1\": 0,\n",
    "                \"val2\": 6,\n",
    "                \"children\": []\n",
    "          },\n",
    "          {\n",
    "                \"name\": \"lsrc\",\n",
    "                \"children\": [\n",
    "                  {\n",
    "                        \"name\": \"core.prefs\",\n",
    "                        \"val1\": 0,\n",
    "                        \"val2\": 18,\n",
    "                        \"children\": []\n",
    "                  },\n",
    "                ]\n",
    "          },\n",
    "      \n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "### JSON Export\n",
    "\n",
    "For exporting the data to JSON you can use the following snippet:\n",
    "\n",
    "```\n",
    "import json\n",
    "\n",
    "with open(\"output.json\", \"w\") as file:\n",
    "    json.dump(tree, file, indent=4)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the main contributor 0> look at github stats:) Shay Banon (kimchy)\n",
    "# UNIX was not tested!!!!\n",
    "import shlex\n",
    "def count_lines_total_and_by_authors(file_path, author_names):\n",
    "    if(os.getcwd() != repo_path):\n",
    "        raise ValueError(\"You must be in the repository folder to run this function\")\n",
    "    \n",
    "    os_type = platform.system()\n",
    "    # Create a grep pattern that matches any of the authors\n",
    "    \n",
    "\n",
    "    # needs system path \n",
    "    # Count total lines in the file\n",
    "    if os_type == 'Windows':\n",
    "        total_lines_cmd = f\"powershell -Command \\\"(Get-Content {file_path} | Measure-Object).Count\\\"\"\n",
    "    else:\n",
    "        total_lines_cmd = f\"wc -l < {file_path}\"\n",
    "\n",
    "    # print()\n",
    "    total_lines_result = subprocess.run(total_lines_cmd, shell=True, capture_output=True, text=True)\n",
    "    total_lines = int(total_lines_result.stdout.strip())\n",
    "\n",
    "    # remove repo path from file path  (fix the slash that is missint at the end of repopath, otherwise reative)\n",
    "    # FIX\n",
    "\n",
    "   \n",
    "    relative_file_path = file_path.replace((repo_path+\"\\\\\"), '')\n",
    "    # Count lines by authors\n",
    "\n",
    "    # needs relative path\n",
    "    if os_type == 'Windows':\n",
    "\n",
    "        patterns = ', '.join([f\"'author {author}'\" for author in author_names])\n",
    "        author_lines_cmd = f\"(git blame --line-porcelain {relative_file_path} | Select-String -Pattern {patterns}).Count\"\n",
    "        author_lines_cmd = f\"powershell -Command \\\"{author_lines_cmd}\\\"\"\n",
    "    else:\n",
    "        grep_pattern = '|'.join([f'^author {shlex.quote(author)}' for author in author_names])\n",
    "        # Unix-like systems: using grep with extended regex\n",
    "        author_lines_cmd = f\"git blame --line-porcelain {relative_file_path} | grep -E '{grep_pattern}' | wc -l\"\n",
    "\n",
    "    # print(author_lines_cmd)\n",
    "   \n",
    "    author_lines_result = subprocess.run(author_lines_cmd, shell=True, capture_output=True, text=True)\n",
    "    author_lines = int(author_lines_result.stdout.strip())\n",
    "    \n",
    "\n",
    "    return total_lines, author_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n",
    "# fp = \"c:\\\\school\\\\schweiz_UNI\\\\fss\\\\fss-se1\\\\se2\\\\elasticsearch\\\\core\\\\src\\\\main\\\\java\\\\org\\\\elasticsearch\\\\Build.java\"\n",
    "# count_lines_total_and_by_authors(fp, [\"kimchy\", \"Shay Banon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6059/6059 [1:58:06<00:00,  1.17s/file]   \n"
     ]
    }
   ],
   "source": [
    "\n",
    "gr = Git(repo_path)\n",
    "commit = gr.get_commit_from_tag(to_tag)\n",
    "gr.checkout(commit.hash)\n",
    "files = defaultdict(lambda: (0 ,0))\n",
    "i = 10\n",
    "# change into repo path\n",
    "os.chdir(repo_path)\n",
    "for file in tqdm(gr.files(), desc=\"Analyzing files\", unit=\"file\"):\n",
    "    # # lowercase first letter of filepath -> windows fix the file is returned with C not c as drive\n",
    "    file = file[0].lower() + file[1:]\n",
    "    total, kimchy = count_lines_total_and_by_authors(file, ['kimchy', 'Shay Banon'])\n",
    "    # print(file)\n",
    "    # print(total, kimchy)\n",
    "    file_relative = file.replace((repo_path+\"\\\\\"), '')\n",
    "    files[file_relative] = (total, kimchy)\n",
    "\n",
    "    # i -= 1\n",
    "    # if(i == 0):\n",
    "    #     break\n",
    "    \n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_json_structure(data):\n",
    "    def add_to_tree(base, parts, values):\n",
    "        for part in parts[:-1]:\n",
    "            found = next((item for item in base if item['name'] == part), None)\n",
    "            if not found:\n",
    "                new_node = {\"name\": part, \"children\": []}\n",
    "                base.append(new_node)\n",
    "                base = new_node['children']\n",
    "            else:\n",
    "                base = found['children']\n",
    "        \n",
    "        # Add the file as a child to the last directory\n",
    "        # values[0] -> total\n",
    "        # values[1] -> kimchy\n",
    "        to_append = {\n",
    "                    \"name\": parts[-1], \n",
    "                    \"author_color\": values[0] != 0 and (values[1]/values[0] > 0.5 and \"red\" or \"green\") or \"green\",\n",
    "                    \"size\": values[0], \n",
    "                    \"weight\": values[0],\n",
    "                    \"ownership\":  values[0] != 0 and values[1]/values[0] or 0\n",
    "                    }\n",
    "        base.append(to_append)\n",
    "\n",
    "    tree = {\"name\": \"root\", \"children\": []}\n",
    "    \n",
    "    # Sort the paths for easier processing\n",
    "    sorted_data = sorted(data.items())\n",
    "\n",
    "    for path, values in sorted_data:\n",
    "        parts = path.split('\\\\')\n",
    "        add_to_tree(tree['children'], parts, values)\n",
    "\n",
    "    return tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now run your local python server and open the knowledge_loss.html file in your browser\n",
      "http://localhost:8000/knowledge_loss.html\n"
     ]
    }
   ],
   "source": [
    "# Generate JSON\n",
    "json_structure = build_json_structure(files)\n",
    "json_output = json.dumps(json_structure, indent=2)\n",
    "\n",
    "with open(\"output.json\", \"w\") as file:\n",
    "    json.dump(json_structure, file, indent=4)\n",
    "\n",
    "print(\"Now run your local python server and open the knowledge_loss.html file in your browser\")\n",
    "print(\"http://localhost:8000/knowledge_loss.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe260b7",
   "metadata": {},
   "source": [
    "## Task 3: Code Churn Analysis\n",
    "\n",
    "The third and last task is to analyze the code churn of the _elasticsearch_ project. For this analysis we look at the code churn, meaning the daily change in the total number of lines of the project.\n",
    "\n",
    "Visualize the code churn over time bucketing the data by day. Remember that you'll need to consider also the days when there are no commits.\n",
    "\n",
    "Look at the churn trend over time, identify one outlier, and for it:\n",
    "\n",
    "* investigate if it was caused by a single or multiple commits (since you are bucketing the data by day)\n",
    "* find the hash of the involved commit(s)\n",
    "* find the involved files, and for each file look at the number of lines added and/or deleted as well as the modification type (addition, deletion, modification, renaming)\n",
    "* look at the commit messages\n",
    "\n",
    "Based on the above, discuss the potential reasons for the outlier and if it should be a reason for concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c94bca-d57f-48e8-9603-0061d372c0f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utils_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
